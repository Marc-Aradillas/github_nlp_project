"ROS + CamOdoCal Hand Eye Calibration\n====================================\n\nThis is a ROS node integrating the Hand Eye Calibration implemented in [CamOdoCal](https://github.com/hengli/camodocal). See this [stack exchange question explaining how Hand Eye Calibration works](http://robotics.stackexchange.com/questions/7163/hand-eye-calibration).\n\nExample uses include determining exact transforms with both positions and orientations of a:\n\n - camera attached to the floor is relative to a robot arm\n - camera attached to a robot arm tip relative to a robot arm\n - set of cameras attached to a moving vehicle (this is what camodocal itself implements)\n - two robot arms bolted together\n\n[keynote presentation explaining many details about hand eye calibration](https://www.icloud.com/keynote/AwBUCAESEJ6BPEHy1-J_58iY9QpDxmMaKWYQ8JT4T8wVxHSfiNn7vMJH1IuI3bnUaJeS8H0P8z768Qw95BLoFg2qMCUCAQEEILObYmsh9SaWe-3YhL6v1kNVeciwzjsktabBmlhsO661#Optimal_Hand_Eye_Calibration) for those that are interested. Practical code and instructions to calibrate your robot can be found below.\n\n![Hand Eye Calibration Basics][1]\n\n![Two Common Solutions to Hand Eye Calibration][2]\n\n![AX=XB Hand Eye Calibration Solution][3]\n\n![AX=ZB Hand Eye Calibration Solution][4]\n\nFeeding data into CamOdoCal\n---------------------------\n\n 1. Each measurement taken at a different time, position, and orientation narrows down the possible transforms that can represent the unknown X\n\n 2. Record a list of many transforms A and B taken between different time steps, or relative to the first time step\n      - Rotations are in AxisAngle = UnitAxis*Angle format, or [x_axis,y_axis,z_axis]*\ud835\udf03_angle \n         - ||UnitAxis||=1\n         - || AxisAngle || = \ud835\udf03_angle\n      - Translations are in the normal [x,y,z] format\n 3. Pass both vectors into EstimateHandEyeScrew()\n 4. Returns X in the form of a 4x4 transform estimate\n\n![Camodocal Hand Eye Calibration Details][5]\n\n  [1]: http://i.stack.imgur.com/7k4D3.jpg\n  [2]: http://i.stack.imgur.com/d4nVb.jpg\n  [3]: http://i.stack.imgur.com/wdOyg.jpg\n  [4]: http://i.stack.imgur.com/zRQ1i.jpg\n  [5]: http://i.stack.imgur.com/Cvc75.jpg\n\n\nWhen using this with a robot arm, move it around to a variety of poses and orientations, make sure any data sources that lag behind settle down, then record each pair of poses between the robot base and the robot tip, and between the eye/camera base and the marker, fiducial, or AR tag it is viewing.\n\nThis will save out a yaml file with the results. Be sure to load the results into your system using the data formatted as a rotation matrix, dual quaternion, or quaternion + translation. Roll Pitch Yaw can degenerate and will often be inaccurate!\n\nInstallation\n------------\n\n### Linux\nAll dependencies can be installed via scripts in the [robotics_setup](https://github.com/ahundt/robotics_setup) repository on `Ubuntu 14.04` or `Ubuntu 16.04`.\n\n### MacOS\n\nOn OS X you can use [homebrew](http://brew.sh) and the [homebrew-robotics](https://github.com/ahundt/homebrew-robotics) tap to install all dependencies.\n\n### ROS (both Linux + MacOS)\n\nOnce you've completed the Linux or MacOS steps, follow normal ros source package installation procedures with catkin build.\n\n### Dependencies\n\nIf installing manually, be sure to follow the instructions for each library as there are specific steps required depending on your OS.\n\n- [ROS indigo or kinetic](ros.org)\n- [OpenCV 2 or 3](opencv.org) with (recommended) nonfree components\n    - Note handeye_calib_camodocal does not call any nonfree components, but some users have had difficulty configuring CMake to compile and install all the other dependencies without them.\n    - OpenCV3 puts their nonfree components in [opencv-contrib](https://github.com/opencv/opencv_contrib).\n- [Eigen3](eigen.tuxfamily.org)\n- [ceres-solver](ceres-solver.org)\n- [glog](https://github.com/google/glog)\n  - If you encounter an error about `providing \"FindGlog.cmake\" in CMAKE_MODULE_PATH`, try installing glog from source.\n- [gflags](https://github.com/gflags/gflags)\n\nExamples\n--------\n\nThere are example pre-recorded transforms in the `example` folder, all config files are expected to be in `handeye_calib_camodocal/launch` folder by default, but if that doesn't work try checking the `~/.ros/` folder.\n\n- [launch/handeye_example.launch](launch/handeye_example.launch)\n    - This configures the files transforms are loaded from and saved to, as well as rostopics if reading live data.\n- [example/TransformPairsOutput.yml](example/TransformPairsOutput.yml)\n    - This contains the set of transforms you record with the launch script, which are input into the solver.\n- [example/CalibratedTransform.yml](example/CalibratedTransform.yml)\n    - This transform is your final results found by the solver.\n\nTo verify that the software is working run:\n\n    roslaunch handeye_calib_camodocal handeye_example.launch\n\nYou should see output like the following:\n\n```\n# INFO: Before refinement: H_12 =\n  -0.962926   -0.156063     0.22004 -0.00802514\n  -0.176531    0.981315  -0.0765322   0.0242905\n  -0.203985   -0.112539   -0.972484   0.0550756\n          0           0           0           1\nCeres Solver Report: Iterations: 89, Initial cost: 1.367791e+01, Final cost: 6.005694e-04, Termination: CONVERGENCE\n# INFO: After refinement: H_12 =\n  -0.980558    0.184959   0.0655414  0.00771561\n  0.0495028  -0.0900424    0.994707   0.0836796\n   0.189881    0.978613   0.0791359 -0.00867321\n          0           0           0           1\nResult from /ee_link to /ar_marker_0:\n  -0.980558    0.184959   0.0655414  0.00771561\n  0.0495028  -0.0900424    0.994707   0.0836796\n   0.189881    0.978613   0.0791359 -0.00867321\n          0           0           0           1\nTranslation (x,y,z) :  0.00771561   0.0836796 -0.00867321\nRotation (w,x,y,z): -0.046193, 0.0871038, 0.672938, 0.733099\n\nResult from /ar_marker_0 to /ee_link:\n  -0.980558    0.184959   0.0655414  0.00771561\n  0.0495028  -0.0900424    0.994707   0.0836796\n   0.189881    0.978613   0.0791359 -0.00867321\n          0           0           0           1\nInverted translation (x,y,z) : 0.00507012 0.0145954 -0.083056\nInverted rotation (w,x,y,z): -0.046193, 0.0871038, 0.672938, 0.733099\n0.046193 0.0871038 0.672938 0.733099\n\nWriting calibration to \"/home/cpaxton/catkin_ws/src/handeye_calib_camodocal/launch/CalibratedTransform.yml\"...\n[handeye_calib_camodocal-1] process has finished cleanly\nlog file: /home/cpaxton/.ros/log/a829db0a-f96b-11e6-b1dd-fc4dd43dd90b/handeye_calib_camodocal-1*.log\nall processes on machine have died, roslaunch will exit\nshutting down processing monitor...\n... shutting down processing monitor complete\ndone\n```\n\nThe full terminal session can be found at:\n\n - [example/terminal_session.txt](example/terminal_session.txt)\n\nRecording your own Transforms\n-----------------------------\n\nTo record your own session, modify [launch/handeye_file.launch](launch/handeye_file.launch) to specify the ROS topics that will publish the poses between which you wish to calibrate, then run:\n\n\n    roslaunch handeye_calib_camodocal handeye_file.launch\n\nIf you have difficulty we cover just about every problem we've seen below in the troubleshooting section. It can also help to see this [stack exchange question explaining how Hand Eye Calibration works](http://robotics.stackexchange.com/questions/7163/hand-eye-calibration)\n\nAfter you run, be sure to back up `TransformPairsInput.yml` and `CalibratedTransform.yml` so you don't lose all\nthe transforms and positions you saved!\n\n#### How do I get transforms between the camera and an object it sees?\n\nIf a camera is calibrated it is possible to estimate the transform from the camera to a printed pattern with known dimensions. I don\u2019t recommend using a checkerboard for hand eye calibration because the pattern is ambiguous. Use something like:\n\n - artoolkit.org\n - https://github.com/ros-perception/ar_track_alvar\n \n They provide instructions on how to set up your camera and create patterns that can be used to generate transforms.\n\nTroubleshooting\n---------------\n\n#### Saved Files Not Loading?\n\nIf you have trouble finding the saved files, the default working directory of ROS applications is in the `~/.ros/` folder, so try looking there. Be sure to also check your launch file which is typically\n[launch/handeye_file.launch](launch/handeye_file.launch) this determines if transforms will be loaded\nfrom a running robot or saved files, as well as where save files are placed.\n\n#### Collecting Enough Data\n\nWe recommend you collect at least ~36 accurate transforms for a good calibration. If it fails to\nconverge (i.e. you don't get a good result out), then you probably have your transforms flipped\nthe wrong way or there is too much noise in your data to find a sufficiently accurate calibration.\n\n### Eliminating Sensor Noise\n\nOne simple method to help deal with this problem is to create a new node that reads the data you want\nto read and save a rolling average of the pose. This helps to stabilize the results. There are better\nmethods such as a kalman filter that could handle this even better. If you take a rolling average,\nmake sure each time you take the data the robot has been set in a single position for the entire duration\nof the time for which the rolling average is being taken, because any error here will throw off the results.\n\n\n### Examples of \"too much noise\" when taking data\n\nIf there is too much noise you will probably see the following error:\n\n```\nnormalization could not be handled. Your rotations and translations are probably either not aligned or not passed in properly\n```\n\nThat means there is probably too much variation in the data you are reading to get an accurate solution.\nFor example, if you watch the pose of an AR tag and it wobbles a little or flips this will prevent an\naccurate solution from being found. One way to help this is to ensure the system is completely stationary and then interpolate (average) the poses across several frames, again ensuring the system is completely stationary before recording the frame and then finally moving to the next position and repeating the process.\n\n#### Your cameras must be calibrated\n\nCamera calibration is  very important! If they aren't calibrated then the poses being fed into the algorithm will be inaccurate, won't correspond, and thus the algorithm won't be able to find even a decent approximate solution and will just exit, printing an error.\n\n#### Your robot and cameras must be rigidly fixed\n\nHand eye calibration solves for a rigid body transform, so if the whole system isn't rigidly fixed the transform you are solving for is constantly changing and thus impossible to find accurately. For example, if you have a camera and a fixed robot base, check that your robot is securely bolted to a surface. Tighten those bolts up! Also ensure the camera is securely and rigidly fixed in place in a similar fasion. Check for any wobbling and make sure to wait for everything to become still before taking your data points.\n\n#### Sanity Check by Physically Measuring\n\nSlight distortion or variation in time stamp while the arm moves slightly as you hold it can still throw it off. One additional way to test that is to have the arm go to two distant positions, and the length of the change in checkerboard poses should be equal to the length of the change in end effector tip poses assuming you can keep the orientation constant.\n\n#### Sanity Check via Simulation\n\nIf you\u2019re concerned it is a bug in the algorithm you can run it in simulation with v-rep or gazebo (os + v-rep python script is in the repo) to verify it works, since that will avoid all physical measurement problems. From there you could consider taking more real data and incorporating the real data to narrow down the source of the problem.\n\n#### Sanity Check Transforms and when loading from files\n\nIf you're loading from a file you've modified by hand, check if your matrices are transposed, inverted, or in very unusual cases even just the 3x3 Rotation component of the 4x4 rotation matrix may be transposed.\n\nExample output\n--------------\n\nHere is an example output of what you should expect when a run is executed successfully:\n\n```\nWriting pairs to \"/home/cpaxton/catkin_ws/src/handeye_calib_camodocal/launch/TransformPairsInput.yml\"...\nq[ INFO] [1473813682.393291696]: Calculating Calibration...\n# INFO: Before refinement: H_12 =\n-0.00160534     0.99916   0.0409473 -0.00813108\n-0.00487176  -0.0409546    0.999149     0.10692\n  0.999987  0.00140449  0.00493341   0.0155885\n         0           0           0           1\nCeres Solver Report: Iterations: 99, Initial cost: 1.882582e-05, Final cost: 1.607494e-05, Termination: CONVERGENCE\n# INFO: After refinement: H_12 =\n-0.00282176     0.999009    0.0444162  -0.00746998\n  0.0121142   -0.0443789     0.998941     0.101617\n   0.999923   0.00335684    -0.011977 -0.000671928\n          0            0            0            1\nResult:\n-0.00282176     0.999009    0.0444162  -0.00746998\n  0.0121142   -0.0443789     0.998941     0.101617\n   0.999923   0.00335684    -0.011977 -0.000671928\n          0            0            0            1\nTranslation:  -0.00746998     0.101617 -0.000671928\nRotation: -0.48498 0.513209 0.492549 0.513209\n```\n\nNote that this run is not a perfect one with errors of 5 mm over a motion of 1 m.\n\n#### Cost\n\nOne key piece of information is the output of the cost function, which is a metric representing an estimate of solution accuracy:\n\n```\nInitial cost: 1.882582e-05, Final cost: 1.607494e-05\n```\n\nWith a really good run where the calibration is dead on the final cost should be on the order of 1e-13 or 1e-14.\n\n#### Results\n\nNow lets take a look at the results:\n\n```\nTranslation:  -0.00746998     0.101617 -0.000671928\nRotation: -0.48498 0.513209 0.492549 0.513209\n```\n\nThe translation is in xyz format, and the rotation is in quaternion format. It is important to note that this tool and camodocal use the eigen Quaternion format which orders the four values stored in a quaternion wxyz. ROS launch files, by comparison store the data in the order xyzw. That means when copying the results into ROS you must move the first entry of the rotation to the end.\n\nHere is an example of all 7 numbers from above correctly put into a ros launch file:\n\n```\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"endpoint_to_marker\" args=\" -0.00746998     0.101617 -0.000671928  0.513209 0.492549 0.513209  -0.48498   $(arg ee_frame) /endpoint_marker 10\"/>\n```\n\nQuestions? Here is what we need to know.\n----------------------------------------\n\nIf you try running this and have a question please create a diagram of your use case so we can understand how you are setting up the equations, then create a [github issue](https://github.com/jhu-lcsr/handeye_calib_camodocal/issues).\n\nSee this [stack exchange question explaining of how Hand Eye Calibration works](http://robotics.stackexchange.com/questions/7163/hand-eye-calibration) for an example of such a diagram.\n\nAuthors\n-------\n\nAndrew Hundt <ATHundt@gmail.com>\nFelix Jonathan <fjonath1@jhu.edu>\n\nAcknowledgements\n----------------\n\n[Hand-Eye Calibration Using Dual Quaternions](https://www.informatik.uni-kiel.de/inf/Sommer/doc/Publications/kd/ijrr99.pdf)\n\n    @article{daniilidis1999hand,\n      title={Hand-eye calibration using dual quaternions},\n      author={Daniilidis, Konstantinos},\n      journal={The International Journal of Robotics Research},\n      volume={18},\n      number={3},\n      pages={286--298},\n      year={1999},\n      publisher={SAGE Publications}\n    }\n\n\n[CamOdoCal](https://github.com/hengli/camodocal)\n\n    Lionel Heng, Bo Li, and Marc Pollefeys,\n    CamOdoCal: Automatic Intrinsic and Extrinsic Calibration of a Rig with Multiple Generic Cameras and Odometry,\n    In Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013.\n\n    Lionel Heng, Mathias B\u00fcrki, Gim Hee Lee, Paul Furgale, Roland Siegwart, and Marc Pollefeys,\n    Infrastructure-Based Calibration of a Multi-Camera Rig,\n    In Proc. IEEE International Conference on Robotics and Automation (ICRA), 2014.\n\n    Lionel Heng, Paul Furgale, and Marc Pollefeys,\n    Leveraging Image-based Localization for Infrastructure-based Calibration of a Multi-camera Rig,\n    Journal of Field Robotics (JFR), 2015.\n    \n  \nReferences\n----------\n\n- Strobl, K., & Hirzinger, G. (2006) . Optimal hand-eye calibration. In 2006 IEEE/RSJ international conference on intelligent robots and systems (pp. 4647\u20134653), October 2006.\n- [Technical University of Munich (TUM) CAMP lab wiki\u2028](http://campar.in.tum.de/Chair/HandEyeCalibration)\n- K. Daniilidis, \u201cHand\u2013Eye Calibration Using Dual Quaternions,\u201d Int. Journal of Robs. Research, vol. 18, no. 3, pp. 286\u2013298, June 1999.\n- E. Bayro\u2013Corrochano, K. Daniilidis, and G. Sommer, \u201cMotor\u2013Algebra for 3D Kinematics: The Case of Hand\u2013Eye Calibration,\u201d Journal for Mathem. Imaging and Vision, vol. 13, no. 2, pp. 79\u2013100, Oct. 2000. \n- F. Dornaika and R. Horaud, \u201cSimultaneous Robot\u2013World and Hand\u2013 Eye Calibration,\u201d IEEE Trans. on Robs. and Aut., vol. 14, no. 4, pp. 617\u2013622, August 1998. \n- Note: figures and text are from mixed sources including the presentation author, the various papers referenced, and the TUM wiki.\n"